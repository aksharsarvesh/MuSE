{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import mysql.connector\n",
    "import ssl\n",
    "import pymysql\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty array to store the data\n",
    "# data = []\n",
    "\n",
    "# Open the CSV file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DELIMITER = \"<BRK>\"\n",
    "                                \n",
    "counter = -1\n",
    "\n",
    "random.seed(17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding this flag so we don't update every time\n",
    "update_songs_flag = False\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    user=os.getenv('DB_USERNAME'),\n",
    "    password=os.getenv('DB_PASSWORD'),\n",
    "    host=os.getenv('DB_HOST'),\n",
    "    port=int(os.getenv('DB_PORT')),\n",
    "    database=os.getenv('DB_NAME'),\n",
    "    ssl={'ca': './ca-certificate.crt'}\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "if update_songs_flag == True:\n",
    "    with open('final_tracks.csv', mode='r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if counter >= 0 and counter > last_committed:\n",
    "                index = counter\n",
    "                name = row[1]\n",
    "                artists = ast.literal_eval(row[2])\n",
    "                artists_str = DELIMITER.join(artists)\n",
    "                song_id = row[3]\n",
    "                popularity = row[4]\n",
    "                artist_ids = ast.literal_eval(row[8])\n",
    "                artist_ids_str = DELIMITER.join(artist_ids)\n",
    "                playlist_ids = ast.literal_eval(row[9])\n",
    "                num_playlists = len(playlist_ids)\n",
    "                playlist_ids_str = DELIMITER.join(playlist_ids)\n",
    "\n",
    "                print(str(index) + \":\", name, \"-\", artists_str)\n",
    "\n",
    "                query = \"\"\"\n",
    "                    INSERT INTO CS_229_SONGS_ALL (SONG_NUM, NAME, ARTISTS, SONG_ID, POPULARITY, ARTIST_ID, PLAYLIST_IDS, NUM_PLAYLISTS)\n",
    "                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                \"\"\"\n",
    "\n",
    "                cursor.execute(query, (index, name, artists_str, song_id,\n",
    "                               popularity, artist_ids_str, playlist_ids_str, num_playlists))\n",
    "                conn.commit()\n",
    "\n",
    "            last_committed = counter\n",
    "            counter += 1\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymysql\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "playlist_set = set()\n",
    "songs_set = set()\n",
    "num_songs = 10\n",
    "song_to_name = {}\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Establish a connection to the database\n",
    "conn = pymysql.connect(\n",
    "    user=os.getenv('DB_USERNAME'),\n",
    "    password=os.getenv('DB_PASSWORD'),\n",
    "    host=os.getenv('DB_HOST'),\n",
    "    port=int(os.getenv('DB_PORT')),\n",
    "    database=os.getenv('DB_NAME')\n",
    ")\n",
    "\n",
    "try:\n",
    "    with conn.cursor() as cursor:\n",
    "        # SQL query to find the top N songs based on NUM_PLAYLISTS\n",
    "        query = f\"\"\"\n",
    "            SELECT SONG_NUM, NAME, NUM_PLAYLISTS, PLAYLIST_IDS, ARTISTS\n",
    "            FROM CS_229_SONGS_ALL\n",
    "            ORDER BY NUM_PLAYLISTS DESC\n",
    "            LIMIT {num_songs}\n",
    "        \"\"\"\n",
    "        cursor.execute(query)\n",
    "\n",
    "        # Fetch all results\n",
    "        top_songs = cursor.fetchall()\n",
    "\n",
    "        # Print results\n",
    "        for song in top_songs:\n",
    "            playlist_ids_list = song[3].split(DELIMITER)\n",
    "            playlist_set.update(set(playlist_ids_list))\n",
    "            songs_set.add(song[0])\n",
    "            print(\n",
    "                f\"Song Number: {song[0]}, Name: {song[1]}, Number of Playlists: {song[2]}\")\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(playlist_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# This dict maps playlist to the songs in those playlists\n",
    "playlist_songs_dict = {}\n",
    "\n",
    "# Processing songs\n",
    "for playlist in playlist_set:\n",
    "    playlist_songs_dict[playlist] = []\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    user=os.getenv('DB_USERNAME'),\n",
    "    password=os.getenv('DB_PASSWORD'),\n",
    "    host=os.getenv('DB_HOST'),\n",
    "    port=int(os.getenv('DB_PORT')),\n",
    "    database=os.getenv('DB_NAME')\n",
    ")\n",
    "\n",
    "try:\n",
    "    with conn.cursor() as cursor:\n",
    "        query = \"\"\"\n",
    "            SELECT SONG_NUM, PLAYLIST_IDS, NAME, ARTISTS\n",
    "            FROM CS_229_SONGS_ALL\n",
    "            ORDER BY NUM_PLAYLISTS DESC\n",
    "        \"\"\"\n",
    "        cursor.execute(query)\n",
    "\n",
    "        all_songs = cursor.fetchall()\n",
    "\n",
    "        for song in all_songs:\n",
    "            add_song_flag = False\n",
    "            for playlist in song[1].split(DELIMITER):\n",
    "                if playlist in playlist_set:\n",
    "                    playlist_songs_dict[playlist].append(song[0])\n",
    "                    add_song_flag = True\n",
    "\n",
    "            if add_song_flag:\n",
    "                songs_set.add(song[0])\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, val, and test sets\n",
    "train_playlists = []\n",
    "train_playlist_set = set() #IDs of playlists as a set\n",
    "train_playlist_songs_dict = {}\n",
    "\n",
    "validation_playlists = []\n",
    "test_playlists = []\n",
    "\n",
    "count = 0 \n",
    "for playlist in playlist_set: \n",
    "    if count % 9 == 0: \n",
    "        test_playlists.append(playlist_songs_dict[playlist])\n",
    "    elif count % 10 == 0: \n",
    "        validation_playlists.append(playlist_songs_dict[playlist])\n",
    "    else: \n",
    "        train_playlists.append(playlist_songs_dict[playlist])\n",
    "        train_playlist_set.add(playlist)\n",
    "        train_playlist_songs_dict[playlist] = playlist_songs_dict[playlist]\n",
    "    count += 1\n",
    "\n",
    "# Generate test and validation sets\n",
    "first_time_flag = True\n",
    "for playlist in train_playlists:\n",
    "    if first_time_flag:\n",
    "        train_songs = set(playlist)\n",
    "        first_time_flag = False\n",
    "    else:\n",
    "        train_songs = train_songs.union(set(playlist))\n",
    "\n",
    "# Generate validation examples for hyperparam tuning eval\n",
    "validation_examples = []\n",
    "while len(validation_examples) <= 100:\n",
    "    for playlist in validation_playlists:\n",
    "        i = random.randint(0, len(playlist) - 2)\n",
    "        if playlist[i] in train_songs and playlist[i + 1] in train_songs:\n",
    "            validation_examples.append(tuple((playlist[i], playlist[i+1], 1)))\n",
    "while len(validation_examples) <= 200:\n",
    "    train_songs_list = list(train_songs)\n",
    "    song_1 = train_songs_list[random.randint(0, len(train_songs_list) - 1)]\n",
    "    song_2 = train_songs_list[random.randint(0, len(train_songs_list) - 1)]\n",
    "    if song_1 != song_2:\n",
    "        pair_good = True\n",
    "        for playlist in validation_playlists:\n",
    "            if song_1 in playlist and song_2 in playlist:\n",
    "                pair_good = False\n",
    "        if pair_good:\n",
    "            validation_examples.append(tuple((song_1, song_2, 0)))\n",
    "\n",
    "# Generate test examples for final evaluation\n",
    "test_examples = []\n",
    "while len(test_examples) <= 100:\n",
    "    for playlist in test_playlists:\n",
    "        i = random.randint(0, len(playlist) - 2)\n",
    "        if playlist[i] in train_songs and playlist[i + 1] in train_songs:\n",
    "            test_examples.append(tuple((playlist[i], playlist[i+1], 1)))\n",
    "while len(test_examples) <= 200:\n",
    "    train_songs_list = list(train_songs)\n",
    "    song_1 = train_songs_list[random.randint(0, len(train_songs_list) - 1)]\n",
    "    song_2 = train_songs_list[random.randint(0, len(train_songs_list) - 1)]\n",
    "    if song_1 != song_2:\n",
    "        pair_good = True\n",
    "        for playlist in test_playlists:\n",
    "            if song_1 in playlist and song_2 in playlist:\n",
    "                pair_good = False\n",
    "    if pair_good:\n",
    "        test_examples.append(tuple((song_1, song_2, 0)))\n",
    "\n",
    "songs_set = train_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# of songs in the database is:\", str(len(songs_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making co-occurrence matrix\n",
    "\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Our vocab is the songs_set\n",
    "# Map song names to indexes\n",
    "song_to_index = {song: index for index, song in enumerate(list(songs_set))}\n",
    "\n",
    "cooccurrence_matrix = lil_matrix((len(songs_set), len(songs_set)), dtype=int)\n",
    "\n",
    "playlist_counter = 0\n",
    "for playlist in train_playlist_set:\n",
    "    # Indexes as row, col in matrix to update, mapped from the songs associated with the playlist\n",
    "\n",
    "    indexes = [song_to_index[song]\n",
    "               for song in train_playlist_songs_dict[playlist]]\n",
    "    for i in range(len(indexes)):\n",
    "        for j in range(i + 1, len(indexes)):\n",
    "            cooccurrence_matrix[indexes[i], indexes[j]] += 1\n",
    "            cooccurrence_matrix[indexes[j], indexes[i]] += 1\n",
    "    playlist_counter += 1\n",
    "    if playlist_counter % 100 == 0:\n",
    "        print(\"Finished processing playlist #\" + str(playlist_counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks on the co-occurrence matrix\n",
    "cooccurrence_matrix = cooccurrence_matrix.tocsr()\n",
    "print(\"Co-occurrence matrix shape:\", cooccurrence_matrix.shape)\n",
    "print(\"Number of non-zero entries:\", cooccurrence_matrix.nnz)\n",
    "print(\"Number of zero entries:\", str(108338 ** 2 - cooccurrence_matrix.nnz))\n",
    "\n",
    "row_sums = cooccurrence_matrix.sum(axis=1)\n",
    "column_sums = cooccurrence_matrix.sum(axis=0)\n",
    "\n",
    "row_sums = np.array(row_sums).flatten()\n",
    "column_sums = np.array(column_sums).flatten()\n",
    "\n",
    "print(\"Row sums (first 10):\", row_sums[:10])\n",
    "print(\"Column sums (first 10):\", column_sums[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Live functions to initialize training and let it run. \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def init_embeddings(num_songs, dim=25):\n",
    "    embeddings = np.random.randn(num_songs, dim) * 0.01\n",
    "    bias = np.zeros(num_songs)\n",
    "    return embeddings, bias\n",
    "\n",
    "def weighting_func(x, x_max, alpha=0.75):\n",
    "    return np.where(x < x_max, (x / x_max) ** alpha, 1)\n",
    "\n",
    "def basic_train(cooccurrence_matrix, vocab_size, emb_dim, lr=0.05, epochs=150, x_max=1000, alpha=0.75):\n",
    "    # Initialize embeddings and bias\n",
    "    embeddings, bias = init_embeddings(vocab_size, emb_dim)\n",
    "    \n",
    "    # Extract non-zero co-occurrence pairs (i, j, x_ij)\n",
    "    coo_matrix = cooccurrence_matrix.tocoo()  # Convert sparse matrix to COO format (for easy iteration)\n",
    "    non_zero_indices = list(zip(coo_matrix.row, coo_matrix.col, coo_matrix.data))\n",
    "    \n",
    "    # Precompute weights for the non-zero elements\n",
    "    cooccurrence_values = np.array([x[2] for x in non_zero_indices])\n",
    "    weights = weighting_func(cooccurrence_values, x_max, alpha)\n",
    "    \n",
    "    total_loss_history = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for idx, (i, j, x_ij) in enumerate(non_zero_indices):\n",
    "            # Compute dot product + bias\n",
    "            prediction = np.dot(embeddings[i], embeddings[j]) + bias[i] + bias[j]\n",
    "            log_x_ij = np.log(x_ij + 1)  # Laplace Smoothing\n",
    "            weight = weights[idx]\n",
    "            \n",
    "            # Compute the loss for this pair\n",
    "            loss = weight * (prediction - log_x_ij) ** 2\n",
    "            total_loss += loss\n",
    "            \n",
    "            # Compute gradients\n",
    "            grad_common = 2 * weight * (prediction - log_x_ij)\n",
    "            grad_emb_i = grad_common * embeddings[j]\n",
    "            grad_emb_j = grad_common * embeddings[i]\n",
    "            grad_bias_i = grad_common\n",
    "            grad_bias_j = grad_common\n",
    "            \n",
    "            # Update embeddings and biases\n",
    "            embeddings[i] -= lr * grad_emb_i\n",
    "            embeddings[j] -= lr * grad_emb_j\n",
    "            bias[i] -= lr * grad_bias_i\n",
    "            bias[j] -= lr * grad_bias_j\n",
    "        \n",
    "        total_loss_history.append(total_loss)\n",
    "        print(f\"Epoch {epoch} Loss: {total_loss}\")\n",
    "        \n",
    "        # Convergence check\n",
    "        if epoch > 0 and np.abs(total_loss_history[-1] - total_loss_history[-2]) < 1e-4:\n",
    "            print(f\"Convergence reached at epoch {epoch + 1}.\")\n",
    "            break\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for analysis of which songs to plot\n",
    "\"\"\"for key in song_to_name_embedding_dict.keys(): \n",
    "    print(key, song_to_name_embedding_dict[key])\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_embeddings(embeddings, val, test): \n",
    "    # Takes ~7 secs ish to run per 1k songs: sets up indexing in order to interpret tSNE\n",
    "    # Note that we only need to do this once, do all embedding training under this!!!!\n",
    "    load_dotenv()\n",
    "\n",
    "    conn = pymysql.connect(\n",
    "        user=os.getenv('DB_USERNAME'),\n",
    "        password=os.getenv('DB_PASSWORD'),\n",
    "        host=os.getenv('DB_HOST'),\n",
    "        port=int(os.getenv('DB_PORT')),\n",
    "        database=os.getenv('DB_NAME')\n",
    "    )\n",
    "\n",
    "    # Reverses song_to_index (song ID: matrix index)- now is matrix index: song ID\n",
    "    index_to_song = {index: song for song, index in song_to_index.items()}\n",
    "    song_to_name_embedding_dict = {}\n",
    "\n",
    "    for song_id in song_to_index.keys():\n",
    "        with conn.cursor() as cursor:\n",
    "            query = f\"\"\"\n",
    "                SELECT NAME, ARTISTS\n",
    "                FROM CS_229_SONGS_ALL\n",
    "                WHERE SONG_NUM = {song_id}\n",
    "                LIMIT {1}\n",
    "            \"\"\"\n",
    "            cursor.execute(query)\n",
    "            top_songs = cursor.fetchall()\n",
    "        artists = top_songs[0][1].split(DELIMITER)\n",
    "        song_to_name_embedding_dict[song_id] = tuple(\n",
    "            (top_songs[0][0], artists, embeddings[song_to_index[song_id]]))\n",
    "        if len(song_to_name_embedding_dict.keys()) % 1000 == 0:\n",
    "            print(\"Done processing\", str(len(song_to_name_embedding_dict.keys())), \"songs\")\n",
    "\n",
    "    print(song_to_name_embedding_dict)\n",
    "\n",
    "    selected_song_ids_1 = [816, 817, 58385, 862, 45032, 51494, 24175, 974, 72162, 955, 9048, 9082]\n",
    "    # LADY GAGA: 816 - Just Dance, 817 - Paparazzi, 58385 - Applause\n",
    "    # JUSTIN BIEBER: 862 - Ghost, 45032 - Off My Face, 51494 - 2U\n",
    "    # KENDRICK LAMAR: 24175 - Alright, 974 - PRIDE., 72162 - Rigamortus\n",
    "    # PITBULL: 955 - Time of Our Lives, 9048 - Timber, 9082 - Fireball\n",
    "\n",
    "    selected_song_ids_2 = [142040, 13378, 13402, 74386, 74387, 75851, 72303, 47208, 53778, 259796, 4993, 5839]\n",
    "    # LUKE COMBS (country) - 142040 - One Number Away, 13378 - Beautiful Crazy, 13402 - The Kind of Love We Make \n",
    "    # HAMILTON - 74386 - I Know Him, 74387 - What Comes Next?, 75851 - Wait For It \n",
    "    # BAD BUNNY - 72303 - Neverita, 47208 - Titi me Pregunto, 53778 - Safaera \n",
    "    # QUEEN - 259796 - Radio Ga Ga, 4993 - Under Pressure, 5839 - Don't Stop Me Now\n",
    "\n",
    "    if val: \n",
    "        get_val_accuracy(embeddings)\n",
    "    if test: \n",
    "        get_test_accuracy(embeddings)\n",
    "\n",
    "    generate_visual(song_to_name_embedding_dict, selected_song_ids_1)\n",
    "    generate_visual(song_to_name_embedding_dict, selected_song_ids_2)\n",
    "    return song_to_name_embedding_dict\n",
    "\n",
    "def get_val_accuracy(embeddings): \n",
    "    total_examples = len(test_examples)\n",
    "    correct_sum = 0  \n",
    "    count = 0\n",
    "\n",
    "    for ex in validation_examples:\n",
    "        song_1, song_2, label = ex[0], ex[1], ex[2]\n",
    "        embedding_1 = embeddings[song_to_index[song_1]]\n",
    "        embedding_2 = embeddings[song_to_index[song_2]]\n",
    "        cosine_sim = np.dot(embedding_1, embedding_2) / (np.linalg.norm(embedding_1) * np.linalg.norm(embedding_2))\n",
    "        if cosine_sim > 0.2: \n",
    "            if ex[2] == 1: \n",
    "                correct_sum += 1\n",
    "            count += 1\n",
    "        elif cosine_sim < -0.2: \n",
    "            if ex[2] == 0: \n",
    "                correct_sum += 1\n",
    "            count += 1\n",
    "    print(f\"Validation accuracy is {correct_sum} / {count} = {correct_sum / count}\")\n",
    "        \n",
    "\n",
    "def get_test_accuracy(embeddings): \n",
    "    total_examples = len(test_examples)\n",
    "    correct_sum = 0\n",
    "    count = 0\n",
    "    for ex in test_examples: \n",
    "        song_1, song_2, label = ex[0], ex[1], ex[2]\n",
    "        embedding_1 = embeddings[song_to_index[song_1]]\n",
    "        embedding_2 = embeddings[song_to_index[song_2]]\n",
    "        cosine_sim = np.dot(embedding_1, embedding_2) / (np.linalg.norm(embedding_1) * np.linalg.norm(embedding_2))\n",
    "        if cosine_sim > 0.2:\n",
    "            if ex[2] == 1:\n",
    "                correct_sum += 1\n",
    "            count += 1\n",
    "        elif cosine_sim < -0.2:\n",
    "            if ex[2] == 0:\n",
    "                correct_sum += 1\n",
    "            count += 1\n",
    "    print(f\"Test accuracy is {correct_sum} / {count} = {correct_sum / count}\")\n",
    "\n",
    "\n",
    "def generate_visual(song_to_name_embedding_dict, song_ids): \n",
    "    checkpoint_embeddings_dict = song_to_name_embedding_dict\n",
    "\n",
    "    # Get the corresponding embeddings\n",
    "    selected_embeddings = [checkpoint_embeddings_dict[song_id][2]\n",
    "                        for song_id in song_ids]\n",
    "\n",
    "    # Perform t-SNE to reduce dimensions to 2D, set perplexity lower than the number of samples\n",
    "    tsne = TSNE(n_components=2, random_state=42,\n",
    "                perplexity=10)  # Setting perplexity to 10 < 15\n",
    "    embeddings_2d = tsne.fit_transform(np.array(selected_embeddings))\n",
    "\n",
    "    # Create a dictionary for labels (song IDs to song names)\n",
    "    labels = {\n",
    "        song_id: checkpoint_embeddings_dict[song_id][0] + \" - \" + \", \".join(checkpoint_embeddings_dict[song_id][1]) for song_id in song_ids}\n",
    "\n",
    "    # Plot the reduced 2D embeddings\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=50, cmap='viridis')\n",
    "\n",
    "    # Add text labels for each point, indexed by their position in selected_song_ids\n",
    "    for i, song_id in enumerate(song_ids):\n",
    "        plt.text(embeddings_2d[i, 0], embeddings_2d[i, 1],\n",
    "                labels[song_id], fontsize=9, ha='right', color='red')\n",
    "\n",
    "    # Add plot details\n",
    "    plt.title('2D t-SNE Visualization of Embeddings')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For hyperparam tuning, set epochs = 150\n",
    "# Grid search on:\n",
    "# (a) vector len = 25, 75, 150 -->\n",
    "# (b) alpha = 0.65, 0.75, 0.85\n",
    "# (c) lr = 0.025, 0.05, 0.1\n",
    "\n",
    "\"\"\"\n",
    "ALREADY TRAINED, DO NOT RE-RUN THIS CELL!\n",
    "\"\"\"\n",
    "embeddings_dim_25 = basic_train(\n",
    "    cooccurrence_matrix, cooccurrence_matrix.shape[0], emb_dim=25, alpha=0.75, lr=0.05)\n",
    "evaluate_embeddings(embeddings_dim_25, True, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_val_accuracy(embeddings_dim_25)\n",
    "get_test_accuracy(embeddings_dim_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim_75 = basic_train(\n",
    "    cooccurrence_matrix, cooccurrence_matrix.shape[0], emb_dim=75, alpha=0.75, lr=0.05)\n",
    "evaluate_embeddings(embeddings_dim_75, True, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_val_accuracy(embeddings_dim_75)\n",
    "get_test_accuracy(embeddings_dim_75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim_150 = basic_train(\n",
    "    cooccurrence_matrix, cooccurrence_matrix.shape[0], emb_dim=150, alpha=0.75, lr=0.05)\n",
    "evaluate_embeddings(embeddings_dim_150, True, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ALREADY TRAINED, DO NOT RE-RUN THIS CELL!\n",
    "\"\"\"\n",
    "embeddings_dim_250 = basic_train(\n",
    "    cooccurrence_matrix, cooccurrence_matrix.shape[0], emb_dim=250, alpha=0.75, lr=0.05)\n",
    "evaluate_embeddings(embeddings_dim_250, True, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_val_accuracy(embeddings_dim_250)\n",
    "get_test_accuracy(embeddings_dim_250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim_500 = basic_train(\n",
    "    cooccurrence_matrix, cooccurrence_matrix.shape[0], emb_dim=500, alpha=0.75, lr=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_dict = evaluate_embeddings(embeddings_dim_500, True, False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
