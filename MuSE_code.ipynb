{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Musical Semantic Embeddings (MuSE)\n",
    "# CS 229 - Final Project\n",
    "# Eric Lee and Akshar Sarvesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import statements and initializations for access to database.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import csv\n",
    "import ast\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import mysql.connector\n",
    "import ssl\n",
    "import pymysql\n",
    "import random\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "\n",
    "# Load .env file to access data in MySQL database\n",
    "load_dotenv()\n",
    "\n",
    "# Set delimiter for storing and parsing lists in MySQL database\n",
    "DELIMITER = \"<BRK>\"\n",
    "\n",
    "counter = -1\n",
    "\n",
    "# Fix seed for debugging\n",
    "random.seed(17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load songs from .csv files into MySQL database hosted in DigitalOcean.\n",
    "\"\"\"\n",
    "\n",
    "# Adding this flag so we don't update every time this cell is run. \n",
    "update_songs_flag = False\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    user=os.getenv('DB_USERNAME'),\n",
    "    password=os.getenv('DB_PASSWORD'),\n",
    "    host=os.getenv('DB_HOST'),\n",
    "    port=int(os.getenv('DB_PORT')),\n",
    "    database=os.getenv('DB_NAME'),\n",
    "    ssl={'ca': './ca-certificate.crt'}\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# If flag is active, then parse tracks from .csv and add into database.\n",
    "if update_songs_flag == True:\n",
    "    with open('final_tracks.csv', mode='r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if counter >= 0 and counter > last_committed:\n",
    "                index = counter\n",
    "                name = row[1]\n",
    "                artists = ast.literal_eval(row[2])\n",
    "                artists_str = DELIMITER.join(artists)\n",
    "                song_id = row[3]\n",
    "                popularity = row[4]\n",
    "                artist_ids = ast.literal_eval(row[8])\n",
    "                artist_ids_str = DELIMITER.join(artist_ids)\n",
    "                playlist_ids = ast.literal_eval(row[9])\n",
    "                num_playlists = len(playlist_ids)\n",
    "                playlist_ids_str = DELIMITER.join(playlist_ids)\n",
    "                print(str(index) + \":\", name, \"-\", artists_str)\n",
    "                \n",
    "                # Insert into the database and commit the change. \n",
    "                query = \"\"\"\n",
    "                    INSERT INTO CS_229_SONGS_ALL (SONG_NUM, NAME, ARTISTS, SONG_ID, POPULARITY, ARTIST_ID, PLAYLIST_IDS, NUM_PLAYLISTS)\n",
    "                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                \"\"\"\n",
    "                cursor.execute(query, (index, name, artists_str, song_id,\n",
    "                               popularity, artist_ids_str, playlist_ids_str, num_playlists))\n",
    "                conn.commit()\n",
    "\n",
    "            last_committed = counter\n",
    "            counter += 1\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Query the Songs table in database to sample the playlists to use.\n",
    "\"\"\"\n",
    "\n",
    "playlist_set = set()\n",
    "songs_set = set()\n",
    "num_songs = 10\n",
    "song_to_name = {}\n",
    "\n",
    "try:\n",
    "    with conn.cursor() as cursor:\n",
    "        # SQL query to find the top N songs based on NUM_PLAYLISTS\n",
    "        query = f\"\"\"\n",
    "            SELECT SONG_NUM, NAME, NUM_PLAYLISTS, PLAYLIST_IDS, ARTISTS\n",
    "            FROM CS_229_SONGS_ALL\n",
    "            ORDER BY NUM_PLAYLISTS DESC\n",
    "            LIMIT {num_songs}\n",
    "        \"\"\"\n",
    "        cursor.execute(query)\n",
    "\n",
    "        # Fetch all results\n",
    "        top_songs = cursor.fetchall()\n",
    "\n",
    "        # Print results\n",
    "        for song in top_songs:\n",
    "            playlist_ids_list = song[3].split(DELIMITER)\n",
    "            playlist_set.update(set(playlist_ids_list))\n",
    "            songs_set.add(song[0])\n",
    "            print(\n",
    "                f\"Song Number: {song[0]}, Name: {song[1]}, Number of Playlists: {song[2]}\")\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Iterate through playlists to gather the set of all unique songs within\n",
    "those playlists, from the database.\n",
    "\"\"\"\n",
    "\n",
    "# This dict maps playlist to the songs in those playlists.\n",
    "playlist_songs_dict = {}\n",
    "\n",
    "# Processing songs\n",
    "for playlist in playlist_set:\n",
    "    playlist_songs_dict[playlist] = []\n",
    "\n",
    "try:\n",
    "    with conn.cursor() as cursor:\n",
    "        query = \"\"\"\n",
    "            SELECT SONG_NUM, PLAYLIST_IDS, NAME, ARTISTS\n",
    "            FROM CS_229_SONGS_ALL\n",
    "            ORDER BY NUM_PLAYLISTS DESC\n",
    "        \"\"\"\n",
    "        cursor.execute(query)\n",
    "        all_songs = cursor.fetchall()\n",
    "\n",
    "        for song in all_songs:\n",
    "            add_song_flag = False\n",
    "            for playlist in song[1].split(DELIMITER):\n",
    "                if playlist in playlist_set:\n",
    "                    playlist_songs_dict[playlist].append(song[0])\n",
    "                    add_song_flag = True\n",
    "\n",
    "            if add_song_flag:\n",
    "                songs_set.add(song[0])\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split data into training set, validation set, and tests set, as well as \n",
    "generating the examples necessary from the validation/test set for \n",
    "quantitative evaluation. \n",
    "\"\"\"\n",
    "\n",
    "# Create train, val, and test sets\n",
    "train_playlists = []\n",
    "train_playlist_set = set()\n",
    "train_playlist_songs_dict = {}\n",
    "validation_playlists = []\n",
    "test_playlists = []\n",
    "\n",
    "# Randomly sort 10% of playlists into validation, 10% of playlists into test\n",
    "count = 0\n",
    "for playlist in sorted(playlist_set):\n",
    "    if count % 9 == 0:\n",
    "        test_playlists.append(playlist_songs_dict[playlist])\n",
    "    elif count % 10 == 0:\n",
    "        validation_playlists.append(playlist_songs_dict[playlist])\n",
    "    else:\n",
    "        train_playlists.append(playlist_songs_dict[playlist])\n",
    "        train_playlist_set.add(playlist)\n",
    "        train_playlist_songs_dict[playlist] = playlist_songs_dict[playlist]\n",
    "    count += 1\n",
    "\n",
    "# Use random sampling above to create the training set\n",
    "first_time_flag = True\n",
    "for playlist in train_playlists:\n",
    "    if first_time_flag:\n",
    "        train_songs = set(playlist)\n",
    "        first_time_flag = False\n",
    "    else:\n",
    "        train_songs = train_songs.union(set(playlist))\n",
    "\n",
    "# Generate validation examples for hyperparameter tuning evaluation\n",
    "validation_examples = []\n",
    "while len(validation_examples) <= 100:\n",
    "    for playlist in validation_playlists:\n",
    "        i = random.randint(0, len(playlist) - 2)\n",
    "        if playlist[i] in train_songs and playlist[i + 1] in train_songs:\n",
    "            validation_examples.append(tuple((playlist[i], playlist[i+1], 1)))\n",
    "while len(validation_examples) <= 200:\n",
    "    train_songs_list = list(train_songs)\n",
    "    song_1 = train_songs_list[random.randint(0, len(train_songs_list) - 1)]\n",
    "    song_2 = train_songs_list[random.randint(0, len(train_songs_list) - 1)]\n",
    "    if song_1 != song_2:\n",
    "        pair_good = True\n",
    "        for playlist in validation_playlists:\n",
    "            if song_1 in playlist and song_2 in playlist:\n",
    "                pair_good = False\n",
    "        if pair_good:\n",
    "            validation_examples.append(tuple((song_1, song_2, 0)))\n",
    "\n",
    "# Generate test examples for final evaluation\n",
    "test_examples = []\n",
    "while len(test_examples) <= 100:\n",
    "    for playlist in test_playlists:\n",
    "        i = random.randint(0, len(playlist) - 2)\n",
    "        if playlist[i] in train_songs and playlist[i + 1] in train_songs:\n",
    "            test_examples.append(tuple((playlist[i], playlist[i+1], 1)))\n",
    "while len(test_examples) <= 200:\n",
    "    train_songs_list = list(train_songs)\n",
    "    song_1 = train_songs_list[random.randint(0, len(train_songs_list) - 1)]\n",
    "    song_2 = train_songs_list[random.randint(0, len(train_songs_list) - 1)]\n",
    "    if song_1 != song_2:\n",
    "        pair_good = True\n",
    "        for playlist in test_playlists:\n",
    "            if song_1 in playlist and song_2 in playlist:\n",
    "                pair_good = False\n",
    "    if pair_good:\n",
    "        test_examples.append(tuple((song_1, song_2, 0)))\n",
    "\n",
    "songs_set = train_songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating the co-occurrence matrix M, populating from the playlist-song membership\n",
    "data we have.\n",
    "\"\"\"\n",
    "\n",
    "# Note that our vocab is the songs_set, so establish dict to map song names to indexes\n",
    "song_to_index = {song: index for index, song in enumerate(list(songs_set))}\n",
    "\n",
    "# Create co-occurrence matrix as a sparse matrix \n",
    "cooccurrence_matrix = lil_matrix((len(songs_set), len(songs_set)), dtype=int)\n",
    "\n",
    "playlist_counter = 0\n",
    "for playlist in train_playlist_set:\n",
    "    # Indexes as row, col in matrix to update, mapped from the songs associated with the playlist\n",
    "    indexes = [song_to_index[song]\n",
    "               for song in train_playlist_songs_dict[playlist]]\n",
    "    for i in range(len(indexes)):\n",
    "        for j in range(i + 1, len(indexes)):\n",
    "            cooccurrence_matrix[indexes[i], indexes[j]] += 1\n",
    "            cooccurrence_matrix[indexes[j], indexes[i]] += 1\n",
    "    playlist_counter += 1\n",
    "    if playlist_counter % 100 == 0:\n",
    "        print(\"Finished processing playlist #\" + str(playlist_counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sanity checks on the co-occurrence matrix to ensure it is of proper form.\n",
    "\"\"\"\n",
    "\n",
    "# Check shape of co-occurrence matrix. \n",
    "cooccurrence_matrix = cooccurrence_matrix.tocsr()\n",
    "print(\"Co-occurrence matrix shape:\", cooccurrence_matrix.shape)\n",
    "print(\"Number of non-zero entries:\", cooccurrence_matrix.nnz)\n",
    "print(\"Number of zero entries:\", str(108338 ** 2 - cooccurrence_matrix.nnz))\n",
    "\n",
    "# Ensure the values within the matrix make sense.\n",
    "row_sums = cooccurrence_matrix.sum(axis=1)\n",
    "column_sums = cooccurrence_matrix.sum(axis=0)\n",
    "row_sums = np.array(row_sums).flatten()\n",
    "column_sums = np.array(column_sums).flatten()\n",
    "print(\"Row sums (first 10):\", row_sums[:10])\n",
    "print(\"Column sums (first 10):\", column_sums[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize the embeddings before training.\n",
    "\"\"\"\n",
    "def init_embeddings(num_songs, dim=25):\n",
    "    embeddings = np.random.randn(num_songs, dim) * 0.01\n",
    "    bias = np.zeros(num_songs)\n",
    "    return embeddings, bias\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Establish weighing function with scaling by alpha for embedding generation. \n",
    "\"\"\"\n",
    "def weighting_func(x, x_max, alpha=0.75):\n",
    "    return np.where(x < x_max, (x / x_max) ** alpha, 1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Establish training loop with batch gradient descent working on co-occurrence matrix. \n",
    "\"\"\"\n",
    "def basic_train(cooccurrence_matrix, vocab_size, emb_dim, lr=0.05, epochs=150, x_max=1000, alpha=0.75):\n",
    "    \n",
    "    # Initialize embeddings and bias\n",
    "    embeddings, bias = init_embeddings(vocab_size, emb_dim)\n",
    "\n",
    "    # Extract non-zero co-occurrence pairs (i, j, x_ij)\n",
    "    # Convert sparse matrix to COO format (for easy iteration)\n",
    "    coo_matrix = cooccurrence_matrix.tocoo()\n",
    "    non_zero_indices = list(\n",
    "        zip(coo_matrix.row, coo_matrix.col, coo_matrix.data))\n",
    "\n",
    "    # Precompute weights for the non-zero elements\n",
    "    cooccurrence_values = np.array([x[2] for x in non_zero_indices])\n",
    "    weights = weighting_func(cooccurrence_values, x_max, alpha)\n",
    "    total_loss_history = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for idx, (i, j, x_ij) in enumerate(non_zero_indices):\n",
    "            # Compute dot product + bias\n",
    "            prediction = np.dot(\n",
    "                embeddings[i], embeddings[j]) + bias[i] + bias[j]\n",
    "            log_x_ij = np.log(x_ij + 1)  # Laplace Smoothing\n",
    "            weight = weights[idx]\n",
    "\n",
    "            # Compute the loss for this pair\n",
    "            loss = weight * (prediction - log_x_ij) ** 2\n",
    "            total_loss += loss\n",
    "\n",
    "            # Compute gradients\n",
    "            grad_common = 2 * weight * (prediction - log_x_ij)\n",
    "            grad_emb_i = grad_common * embeddings[j]\n",
    "            grad_emb_j = grad_common * embeddings[i]\n",
    "            grad_bias_i = grad_common\n",
    "            grad_bias_j = grad_common\n",
    "\n",
    "            # Update embeddings and biases\n",
    "            embeddings[i] -= lr * grad_emb_i\n",
    "            embeddings[j] -= lr * grad_emb_j\n",
    "            bias[i] -= lr * grad_bias_i\n",
    "            bias[j] -= lr * grad_bias_j\n",
    "\n",
    "        total_loss_history.append(total_loss)\n",
    "        print(f\"Epoch {epoch} Loss: {total_loss}\")\n",
    "\n",
    "        # Convergence check\n",
    "        if epoch > 0 and np.abs(total_loss_history[-1] - total_loss_history[-2]) < 1e-5:\n",
    "            print(f\"Convergence reached at epoch {epoch + 1}.\")\n",
    "            break\n",
    "    \n",
    "    # Save embeddings for easy future evaluation\n",
    "    os.makedirs(\"basic_embeddings\", exist_ok=True)\n",
    "    np.save(\n",
    "        f\"basic_embeddings/basic_song_embeddings_dim_{emb_dim}.npy\", embeddings)\n",
    "    np.save(f\"basic_embeddings/basic_song_bias_dim_{emb_dim}.npy\", bias)\n",
    "    print(\"Embeddings and biases saved to 'embeddings/' directory.\")\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Establishing functions for quantitative evaluation and qualitative evalution. \n",
    "\"\"\"\n",
    "\n",
    "# Minimum absolute threshold for cosine similarity for meaningful prediction\n",
    "TEST_EPSILON = 0.05\n",
    "\n",
    "\"\"\"\n",
    "Sets up indexing in order to generate t-SNE plot and show points in 2-D.\n",
    "\"\"\"\n",
    "def evaluate_embeddings(embeddings, val, test):\n",
    "    # Note that we only need to do this once per embedding after training.\n",
    "    load_dotenv()\n",
    "    conn = pymysql.connect(\n",
    "        user=os.getenv('DB_USERNAME'),\n",
    "        password=os.getenv('DB_PASSWORD'),\n",
    "        host=os.getenv('DB_HOST'),\n",
    "        port=int(os.getenv('DB_PORT')),\n",
    "        database=os.getenv('DB_NAME')\n",
    "    )\n",
    "\n",
    "    # Reverses song_to_index (song ID: matrix index)- now is matrix index: song ID\n",
    "    index_to_song = {index: song for song, index in song_to_index.items()}\n",
    "    song_to_name_embedding_dict = {}\n",
    "    \n",
    "    # Query database for song and artist name information.\n",
    "    for song_id in song_to_index.keys():\n",
    "        with conn.cursor() as cursor:\n",
    "            query = f\"\"\"\n",
    "                SELECT NAME, ARTISTS\n",
    "                FROM CS_229_SONGS_ALL\n",
    "                WHERE SONG_NUM = {song_id}\n",
    "                LIMIT {1}\n",
    "            \"\"\"\n",
    "            cursor.execute(query)\n",
    "            top_songs = cursor.fetchall()\n",
    "        artists = top_songs[0][1].split(DELIMITER)\n",
    "        song_to_name_embedding_dict[song_id] = tuple(\n",
    "            (top_songs[0][0], artists, embeddings[song_to_index[song_id]]))\n",
    "        if len(song_to_name_embedding_dict.keys()) % 1000 == 0:\n",
    "            print(\"Done processing\", str(\n",
    "                len(song_to_name_embedding_dict.keys())), \"songs\")\n",
    "\n",
    "    \"\"\"\n",
    "    - LADY GAGA: 816 - Just Dance, 817 - Paparazzi, 58385 - Applause\n",
    "    - JUSTIN BIEBER: 862 - Ghost, 45032 - Off My Face, 51494 - 2U\n",
    "    - KENDRICK LAMAR: 24175 - Alright, 974 - PRIDE., 72162 - Rigamortus\n",
    "    - PITBULL: 955 - Time of Our Lives, 9048 - Timber, 9082 - Fireball\n",
    "    \"\"\"\n",
    "    selected_song_ids_1 = [816, 817, 58385, 862, 45032, 51494, 24175, 974, 72162, 955, 9048, 9082]\n",
    "\n",
    "    # Evaluate quantitative metrics if specified as arguments. \n",
    "    if val:\n",
    "        get_val_accuracy(embeddings)\n",
    "    if test:\n",
    "        get_test_accuracy(embeddings)\n",
    "\n",
    "    # Generate t-SNE plots.\n",
    "    generate_visual(song_to_name_embedding_dict, selected_song_ids_1)\n",
    "    return song_to_name_embedding_dict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generate confusion matrix metrics on the validation set, given a set of embeddings. \n",
    "\"\"\"\n",
    "def get_val_accuracy(embeddings):\n",
    "    total_examples = len(test_examples)\n",
    "    true_positive_sum = 0\n",
    "    true_negative_sum = 0\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "\n",
    "    # Iterate through validation examples to generate and check predicted labels. \n",
    "    for ex in validation_examples:\n",
    "        song_1, song_2, label = ex[0], ex[1], ex[2]\n",
    "        embedding_1 = embeddings[song_to_index[song_1]]\n",
    "        embedding_2 = embeddings[song_to_index[song_2]]\n",
    "        cosine_sim = np.dot(embedding_1, embedding_2) / (np.linalg.norm(embedding_1) * np.linalg.norm(embedding_2))\n",
    "        if cosine_sim > TEST_EPSILON:\n",
    "            if ex[2] == 1:\n",
    "                true_positive_sum += 1\n",
    "            positive_count += 1\n",
    "        elif cosine_sim < -1 * TEST_EPSILON:\n",
    "            if ex[2] == 0:\n",
    "                true_negative_sum += 1\n",
    "            negative_count += 1\n",
    "\n",
    "    # Count positives and negatives to generate confusion matrix metrics.\n",
    "    total_correct = true_positive_sum + true_negative_sum\n",
    "    total_count = positive_count + negative_count\n",
    "    precision = true_positive_sum / positive_count\n",
    "    recall = true_positive_sum / \\\n",
    "        (true_positive_sum + (negative_count - true_negative_sum))\n",
    "    print(f\"Validation accuracy is {total_correct / total_count}\")\n",
    "    print(f\"Validation precision is {precision}\")\n",
    "    print(f\"Validation recall is {recall}\")\n",
    "    print(f\"Validation F1 is {(2 * precision * recall) / (precision + recall)}\")\n",
    "    return (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generate confusion matrix metrics on the test set, given a set of embeddings. \n",
    "\"\"\"\n",
    "def get_test_accuracy(embeddings):\n",
    "    total_examples = len(test_examples)\n",
    "    true_positive_sum = 0\n",
    "    true_negative_sum = 0\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "\n",
    "    # Iterate through validation examples to generate and check predicted labels.\n",
    "    for ex in test_examples:\n",
    "        song_1, song_2, label = ex[0], ex[1], ex[2]\n",
    "        embedding_1 = embeddings[song_to_index[song_1]]\n",
    "        embedding_2 = embeddings[song_to_index[song_2]]\n",
    "        cosine_sim = np.dot(embedding_1, embedding_2) / (np.linalg.norm(embedding_1) * np.linalg.norm(embedding_2))\n",
    "        if cosine_sim > TEST_EPSILON:\n",
    "            if ex[2] == 1:\n",
    "                true_positive_sum += 1\n",
    "            positive_count += 1\n",
    "        elif cosine_sim < -1 * TEST_EPSILON:\n",
    "            if ex[2] == 0:\n",
    "                true_negative_sum += 1\n",
    "            negative_count += 1\n",
    "\n",
    "    # Count positives and negatives to generate confusion matrix metrics.\n",
    "    total_correct = true_positive_sum + true_negative_sum\n",
    "    total_count = positive_count + negative_count\n",
    "    precision = true_positive_sum / positive_count\n",
    "    recall = true_positive_sum / \\\n",
    "        (true_positive_sum + (negative_count - true_negative_sum))\n",
    "    print(f\"Test accuracy is {total_correct} / {total_count} = {total_correct / total_count}\")\n",
    "    print(f\"Test precision is {precision}\")\n",
    "    print(f\"Test recall is {recall}\")\n",
    "    print(f\"Test F1 is {(2 * precision * recall) / (precision + recall)}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reduces the dimensionality of embeddings to 2-D with t-SNE and plots.\n",
    "\"\"\"\n",
    "def generate_visual(song_to_name_embedding_dict, song_ids):\n",
    "    checkpoint_embeddings_dict = song_to_name_embedding_dict\n",
    "\n",
    "    # Get the corresponding embeddings\n",
    "    selected_embeddings = [checkpoint_embeddings_dict[song_id][2] for song_id in song_ids]\n",
    "\n",
    "    # Perform t-SNE to reduce dimensions to 2D, set perplexity lower than the number of samples\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=10)  \n",
    "    embeddings_2d = tsne.fit_transform(np.array(selected_embeddings))\n",
    "\n",
    "    # Create a dictionary for labels (song IDs to song names)\n",
    "    labels = {song_id: checkpoint_embeddings_dict[song_id][0] + \" - \" + \", \".join(checkpoint_embeddings_dict[song_id][1]) for song_id in song_ids}\n",
    "\n",
    "    # Plot the reduced 2D embeddings and add text labels\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=50, cmap='viridis')\n",
    "    for i, song_id in enumerate(song_ids):\n",
    "        plt.text(embeddings_2d[i, 0], embeddings_2d[i, 1],\n",
    "                 labels[song_id], fontsize=9, ha='right', color='red')\n",
    "    plt.title('2D t-SNE Visualization of Embeddings')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic embeddings with dimension 25.\n",
    "\"\"\"\n",
    "embeddings_dim_25 = basic_train(cooccurrence_matrix, cooccurrence_matrix.shape[0], emb_dim=25, alpha=0.75, lr=0.05)\n",
    "basic_embeddings_25 = np.load(\"basic_embeddings/basic_song_embeddings_dim_25.npy\")\n",
    "evaluate_embeddings(basic_embeddings_25, True, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic embeddings with dimension 75.\n",
    "\"\"\"\n",
    "embeddings_dim_75 = basic_train(cooccurrence_matrix, cooccurrence_matrix.shape[0], emb_dim=25, alpha=0.75, lr=0.05)\n",
    "basic_embeddings_75 = np.load(\"basic_embeddings/basic_song_embeddings_dim_75.npy\")\n",
    "evaluate_embeddings(basic_embeddings_75, True, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic embeddings with dimension 150.\n",
    "\"\"\"\n",
    "embeddings_dim_150 = basic_train(cooccurrence_matrix, cooccurrence_matrix.shape[0], emb_dim=25, alpha=0.75, lr=0.05)\n",
    "basic_embeddings_150 = np.load(\"basic_embeddings/basic_song_embeddings_dim_150.npy\")\n",
    "evaluate_embeddings(basic_embeddings_150, True, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic embeddings with dimension 250.\n",
    "\"\"\"\n",
    "embeddings_dim_250 = basic_train(cooccurrence_matrix, cooccurrence_matrix.shape[0], emb_dim=25, alpha=0.75, lr=0.05)\n",
    "basic_embeddings_250 = np.load(\"basic_embeddings/basic_song_embeddings_dim_250.npy\")\n",
    "evaluate_embeddings(basic_embeddings_250, True, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic embeddings with dimension 500.\n",
    "\"\"\"\n",
    "embeddings_dim_500 = basic_train(cooccurrence_matrix, cooccurrence_matrix.shape[0], emb_dim=25, alpha=0.75, lr=0.05)\n",
    "basic_embeddings_500 = np.load(\"basic_embeddings/basic_song_embeddings_dim_500.npy\")\n",
    "evaluate_embeddings(basic_embeddings_500, True, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating augmented co-occurrence matrix with artist playlists added to training set.\n",
    "\"\"\"\n",
    "# Copy co-occurrence matrix to increase counts for augmentation.\n",
    "augmented_cooccurrence_matrix = cooccurrence_matrix.copy().tolil()\n",
    "\n",
    "# Iterate through all artists to create artist playlists. \n",
    "artists_dict = {}\n",
    "for song in all_songs:\n",
    "    song_id, playlist_ids, song_name, song_artists = song[0], song[1], song[2], song[3]\n",
    "    if song_id in song_to_index.keys():\n",
    "        artists_list = song_artists.split('<BRK>')\n",
    "        for artist in artists_list:\n",
    "            if artist not in artists_dict.keys():\n",
    "                artists_dict[artist] = []\n",
    "            artists_dict[artist].append(song_id)\n",
    "print(\"Done creating augmented artist playlists!\")\n",
    "print(f\"We have {len(artists_dict.keys())} artists to analyze\")\n",
    "\n",
    "# Iterate through all artist playlists to increment the augmented co-occurrence matrix. \n",
    "new_playlist_counter = 0\n",
    "for key in artists_dict.keys():\n",
    "    indexes = [song_to_index[song] for song in artists_dict[key]]\n",
    "    if len(indexes) >= 2:\n",
    "        for i in range(len(indexes)):\n",
    "            for j in range(i + 1, len(indexes)):\n",
    "                augmented_cooccurrence_matrix[indexes[i], indexes[j]] += 1\n",
    "                augmented_cooccurrence_matrix[indexes[j], indexes[i]] += 1\n",
    "    new_playlist_counter += 1\n",
    "    if new_playlist_counter % 100 == 0:\n",
    "        print(\"Finished processing playlist #\" + str(new_playlist_counter))\n",
    "print(\"Augmented cooccurrence matrix with artists counts finished and stored in variable `augmented_cooccurrence_matrix`\")\n",
    "\n",
    "augmented_coocurrence_matrix = augmented_cooccurrence_matrix.tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define augmented training function, mostly for the saving path for weights. \n",
    "\"\"\"\n",
    "def aug_train(aug_cooccurrence_matrix, vocab_size, emb_dim, lr=0.05, epochs=150, x_max=1000, alpha=0.75):\n",
    "    # Initialize embeddings and bias\n",
    "    embeddings, bias = init_embeddings(vocab_size, emb_dim)\n",
    "\n",
    "    # Extract non-zero co-occurrence pairs (i, j, x_ij)\n",
    "    # Convert sparse matrix to COO format (for easy iteration)\n",
    "    coo_matrix = cooccurrence_matrix.tocoo()\n",
    "    non_zero_indices = list(\n",
    "        zip(coo_matrix.row, coo_matrix.col, coo_matrix.data))\n",
    "\n",
    "    # Precompute weights for the non-zero elements\n",
    "    cooccurrence_values = np.array([x[2] for x in non_zero_indices])\n",
    "    weights = weighting_func(cooccurrence_values, x_max, alpha)\n",
    "\n",
    "    total_loss_history = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for idx, (i, j, x_ij) in enumerate(non_zero_indices):\n",
    "            # Compute dot product + bias\n",
    "            prediction = np.dot(\n",
    "                embeddings[i], embeddings[j]) + bias[i] + bias[j]\n",
    "            log_x_ij = np.log(x_ij + 1)  # Laplace Smoothing\n",
    "            weight = weights[idx]\n",
    "\n",
    "            # Compute the loss for this pair\n",
    "            loss = weight * (prediction - log_x_ij) ** 2\n",
    "            total_loss += loss\n",
    "\n",
    "            # Compute gradients\n",
    "            grad_common = 2 * weight * (prediction - log_x_ij)\n",
    "            grad_emb_i = grad_common * embeddings[j]\n",
    "            grad_emb_j = grad_common * embeddings[i]\n",
    "            grad_bias_i = grad_common\n",
    "            grad_bias_j = grad_common\n",
    "\n",
    "            # Update embeddings and biases\n",
    "            embeddings[i] -= lr * grad_emb_i\n",
    "            embeddings[j] -= lr * grad_emb_j\n",
    "            bias[i] -= lr * grad_bias_i\n",
    "            bias[j] -= lr * grad_bias_j\n",
    "\n",
    "        total_loss_history.append(total_loss)\n",
    "        print(f\"Epoch {epoch} Loss: {total_loss}\")\n",
    "\n",
    "        # Convergence check\n",
    "        if epoch > 0 and np.abs(total_loss_history[-1] - total_loss_history[-2]) < 1e-4:\n",
    "            print(f\"Convergence reached at epoch {epoch + 1}.\")\n",
    "            break\n",
    "\n",
    "    # Store embeddings in new folder dedicated to augmented embeddings.\n",
    "    os.makedirs(\"aug_embeddings\", exist_ok=True)\n",
    "    np.save(\n",
    "        f\"aug_embeddings/aug_song_embeddings_dim_{emb_dim}.npy\", embeddings)\n",
    "    np.save(f\"aug_embeddings/basic_song_bias_dim_{emb_dim}.npy\", bias)\n",
    "    print(\"Embeddings and biases saved to 'aug_embeddings/' directory.\")\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Augmented embeddings with dimension 250.\n",
    "\"\"\"\n",
    "aug_embeddings_dim_250 = aug_train(cooccurrence_matrix, cooccurrence_matrix.shape[0], emb_dim=25, alpha=0.75, lr=0.05)\n",
    "aug_embeddings_250 = np.load(\"aug_embeddings/aug_song_embeddings_dim_250.npy\")\n",
    "evaluate_embeddings(aug_embeddings_250, True, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Augmented embeddings with dimension 500.\n",
    "\"\"\"\n",
    "aug_embeddings_dim_500 = aug_train(cooccurrence_matrix, cooccurrence_matrix.shape[0], emb_dim=25, alpha=0.75, lr=0.05)\n",
    "aug_embeddings_500 = np.load(\"aug_embeddings/aug_song_embeddings_dim_500.npy\")\n",
    "evaluate_embeddings(aug_embeddings_250, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gather playlist titles for contrastive learning generation.\n",
    "\"\"\"\n",
    "# First, make a list of index -> playlist ID. That way we can do list[index] = ID\n",
    "playlist_index_to_ID = dict()\n",
    "\n",
    "# Also, make a playlist index -> title so we can get our BERT weights\n",
    "playlist_index_to_title = dict()\n",
    "\n",
    "# SQL Query to get this information\n",
    "try:\n",
    "    with conn.cursor() as cursor:\n",
    "        query = f\"\"\"SELECT PLAYLIST_ID, NAME FROM CS_229_PLAYLISTS_ALL\"\"\"\n",
    "        cursor.execute(query)\n",
    "        # Fetch all results\n",
    "        playlist_data = cursor.fetchall()\n",
    "        # Print results\n",
    "        index = 0\n",
    "        for playlist in playlist_data:\n",
    "            if playlist[0] in train_playlist_set:\n",
    "                playlist_index_to_ID[index] = playlist[0]\n",
    "                playlist_index_to_title[index] = playlist[1]\n",
    "                index += 1\n",
    "finally:\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "print(len(playlist_index_to_title))\n",
    "print(playlist_index_to_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import BERT models and tokenize.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained model and tokenizer: BERT\n",
    "bertTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bertModel = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Now we get their BERT scores in one np array, and define margin term m\n",
    "n = len(train_playlist_set)\n",
    "m = 768 \n",
    "\n",
    "BERTarray = np.zeros((n, m))\n",
    "for i in range(n):\n",
    "    inputs = bertTokenizer(\n",
    "        playlist_index_to_title[i], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = bertModel(**inputs)\n",
    "        bertData = np.array(outputs.pooler_output)\n",
    "        BERTarray[i] = bertData / np.linalg.norm(bertData)\n",
    "        # Normalize by magnitude so we can dot product directly\n",
    "print(BERTarray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate playlist embeddings by BERT embeddings.\n",
    "\"\"\"\n",
    "# Now we have our bert array, which at each index BERTarray[i] = embedding of playlist of index i\n",
    "playlist_similarity_scores = np.matmul(BERTarray, BERTarray.T) - np.identity(n)\n",
    "print(playlist_similarity_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate candidates for positive and negative pairs for contrastive learning.\n",
    "\"\"\"\n",
    "\n",
    "# Now, flatten the array, argsort to find the best and worst, select some number of them, convert the indices back to 2D\n",
    "bestworst_playlist_count = 100\n",
    "flattened = playlist_similarity_scores.ravel()\n",
    "top_10_indices_flat = np.argpartition(\n",
    "    flattened, -2*bestworst_playlist_count)[-2*bestworst_playlist_count:]  # Largest 20 (unsorted)\n",
    "top_10_indices_flat = top_10_indices_flat[np.argsort(\n",
    "    flattened[top_10_indices_flat])[::-1]]  # Sort descending\n",
    "\n",
    "# Get the indices of the lowest 20 values\n",
    "low_10_indices_flat = np.argpartition(\n",
    "    flattened, 2*bestworst_playlist_count)[:2*bestworst_playlist_count]  # Smallest 20 (unsorted)\n",
    "low_10_indices_flat = low_10_indices_flat[np.argsort(\n",
    "    flattened[low_10_indices_flat])]  # Sort ascending\n",
    "\n",
    "# Convert back to 2D indices\n",
    "top_10_indices_2d = np.unravel_index(\n",
    "    top_10_indices_flat, playlist_similarity_scores.shape)\n",
    "low_10_indices_2d = np.unravel_index(\n",
    "    low_10_indices_flat, playlist_similarity_scores.shape)\n",
    "\n",
    "# Retrieve the top 20 and lowest 20 values\n",
    "top_10_values = flattened[top_10_indices_flat]\n",
    "low_10_values = flattened[low_10_indices_flat]\n",
    "\n",
    "# Combine indices and values for output\n",
    "top_10_results_doubled = list(zip(*top_10_indices_2d))\n",
    "low_10_results_doubled = list(zip(*low_10_indices_2d))\n",
    "top_sim_results = list()\n",
    "low_sim_results = list()\n",
    "for i in range(0, len(top_10_results_doubled), 2):\n",
    "    top_sim_results.append(top_10_results_doubled[i])\n",
    "    low_sim_results.append(low_10_results_doubled[i])\n",
    "    # cut out duplicates\n",
    "\n",
    "print(top_sim_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate positive and negative pairs by sampling from top results.\n",
    "\"\"\"\n",
    "\n",
    "# Sample randomly from each playlist pair of best and worst to create our positive and negative example pairs\n",
    "positive_examples = set()\n",
    "negative_examples = set()\n",
    "examples_coefficient = 20\n",
    "\n",
    "# Iterate through all playlists, pulling from the highest playlist pairs.\n",
    "for i in range(bestworst_playlist_count):\n",
    "    top_playlist1 = playlist_index_to_ID[top_sim_results[i][0]]\n",
    "    top_playlist2 = playlist_index_to_ID[top_sim_results[i][1]]\n",
    "    low_playlist1 = playlist_index_to_ID[low_sim_results[i][0]]\n",
    "    low_playlist2 = playlist_index_to_ID[low_sim_results[i][1]]\n",
    "    for j in range(examples_coefficient):\n",
    "        song_1 = np.random.choice(train_playlist_songs_dict[top_playlist1])\n",
    "        song_2 = np.random.choice(train_playlist_songs_dict[top_playlist2])\n",
    "        if (song_1 in train_songs and song_2 in train_songs):\n",
    "            positive_examples.add((song_1, song_2))\n",
    "        song_1 = np.random.choice(train_playlist_songs_dict[low_playlist1])\n",
    "        song_2 = np.random.choice(train_playlist_songs_dict[low_playlist2])\n",
    "        if (song_1 in train_songs and song_2 in train_songs):\n",
    "            negative_examples.add((song_1, song_2))\n",
    "positive_examples = list(positive_examples)\n",
    "negative_examples = list(negative_examples)\n",
    "print(positive_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define cosine similarity helper function. \n",
    "\"\"\"\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Compute the dot product\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Compute the norms (magnitudes) of the vectors\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n",
    "margin = 0\n",
    "\n",
    "\"\"\"\n",
    "Defines contrastive loss function.\n",
    "\"\"\"\n",
    "def contrastive_loss(embeddings, margin):\n",
    "    loss = 0\n",
    "    for i in range(len(positive_examples)):\n",
    "        song_1_embedding = embeddings[song_to_index[positive_examples[i][0]]]\n",
    "        song_2_embedding = embeddings[song_to_index[positive_examples[i][1]]]\n",
    "        loss += 1 - cosine_similarity(song_1_embedding, song_2_embedding)\n",
    "        # Technically can't guarantee num positive examples = num negative so we'll just loop twice\n",
    "    for i in range(len(negative_examples)):\n",
    "        song_1_embedding = embeddings[song_to_index[negative_examples[i][0]]]\n",
    "        song_2_embedding = embeddings[song_to_index[negative_examples[i][1]]]\n",
    "        loss += max(0, cosine_similarity(song_1_embedding,\n",
    "                    song_2_embedding) - margin)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define contrastive learning loop modified from the original training loop. \n",
    "\"\"\"\n",
    "verbose = False\n",
    "\n",
    "\"\"\"\n",
    "Define the contrastive training function. \n",
    "\"\"\"\n",
    "def train_contrastive(embeddings, threshold, margin):\n",
    "    # Use optimal hyperparameters from tuning. \n",
    "    learningRate = 0.05\n",
    "    epsilon = 1e-7\n",
    "    epochCount = 0\n",
    "    diff = float('inf')\n",
    "    prev = 0\n",
    "    curr = contrastive_loss(embeddings, margin)\n",
    "\n",
    "    # Print extra statements if verbose specified\n",
    "    if verbose:\n",
    "        print(f\"Epoch {epochCount}\")\n",
    "        print(f\"Loss: {curr}\")\n",
    "\n",
    "    # While not converged yet, keep iterating\n",
    "    while diff > threshold:\n",
    "        epochCount += 1\n",
    "        gradients = np.zeros_like(embeddings)\n",
    "        \n",
    "        # Modify embeddings within the positive pairs. \n",
    "        for i in range(len(positive_examples)):\n",
    "            song_1_embedding = embeddings[song_to_index[positive_examples[i][0]]]\n",
    "            song_2_embedding = embeddings[song_to_index[positive_examples[i][1]]]\n",
    "            sim = cosine_similarity(song_1_embedding, song_2_embedding)\n",
    "            gradients[song_to_index[positive_examples[i][0]]] += sim * song_1_embedding / ((np.linalg.norm(song_1_embedding))**2) - (\n",
    "                (song_2_embedding)/(np.linalg.norm(song_1_embedding) * np.linalg.norm(song_2_embedding)))\n",
    "            gradients[song_to_index[positive_examples[i][1]]] += sim * song_2_embedding / ((np.linalg.norm(song_2_embedding))**2) - (\n",
    "                (song_1_embedding)/(np.linalg.norm(song_2_embedding) * np.linalg.norm(song_1_embedding)))\n",
    "        \n",
    "        # Modify embeddings within the negative pairs.\n",
    "        for i in range(len(negative_examples)):\n",
    "            song_1_embedding = embeddings[song_to_index[negative_examples[i][0]]]\n",
    "            song_2_embedding = embeddings[song_to_index[negative_examples[i][1]]]\n",
    "            sim = cosine_similarity(song_1_embedding, song_2_embedding)\n",
    "            if sim > margin:\n",
    "                gradients[song_to_index[positive_examples[i][0]]] -= sim * song_1_embedding / ((np.linalg.norm(song_1_embedding))**2) - (\n",
    "                    (song_2_embedding)/(np.linalg.norm(song_1_embedding) * np.linalg.norm(song_2_embedding)))\n",
    "                gradients[song_to_index[positive_examples[i][1]]] -= sim * song_2_embedding / ((np.linalg.norm(song_2_embedding))**2) - (\n",
    "                    (song_1_embedding)/(np.linalg.norm(song_2_embedding) * np.linalg.norm(song_1_embedding)))\n",
    "        embeddings -= learningRate/(epochCount ** 2) * gradients\n",
    "        prev = curr\n",
    "        curr = contrastive_loss(embeddings, margin)\n",
    "        diff = prev - curr\n",
    "        if verbose and (epochCount % 100 == 0):\n",
    "            print(f\"Epoch {epochCount}\")\n",
    "            print(f\"Loss: {curr}\")\n",
    "            print(f\"Difference: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Grid search on the margin term for hyperparameter tuning.\n",
    "\"\"\"\n",
    "bestEval = 0\n",
    "bestMargin = 0\n",
    "for i in range(-10, 10):\n",
    "    threshold = 1e-4\n",
    "    margin = i/10\n",
    "    contrastive_song_embeddings_dim_500 = np.load(\"basic_embeddings/basic_song_embeddings_dim_500.npy\")\n",
    "    train_contrastive(contrastive_song_embeddings_dim_500, threshold, margin)\n",
    "    print(f\"Margin: {margin}\")\n",
    "    print(\"Contrastive embeddings dim 500:\")\n",
    "    f1 = get_val_accuracy(contrastive_song_embeddings_dim_500)\n",
    "    if (f1 > bestEval):\n",
    "        bestEval = f1\n",
    "        bestMargin = margin\n",
    "        print(bestMargin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete training for contrastive learning for models with length 250 or 500. \n",
    "\"\"\"\n",
    "threshold = 1e-7\n",
    "contrastive_song_embeddings_dim_250 = np.load(\"basic_embeddings/basic_song_embeddings_dim_250.npy\")\n",
    "contrastive_song_embeddings_dim_500 = np.load(\"basic_embeddings/basic_song_embeddings_dim_500.npy\")\n",
    "\n",
    "print(f\"Best Margin: {bestMargin}\")\n",
    "train_contrastive(contrastive_song_embeddings_dim_250, threshold, bestMargin)\n",
    "train_contrastive(contrastive_song_embeddings_dim_500, threshold, bestMargin)\n",
    "\n",
    "# Train the final model which integrates both contrastive and augmentation\n",
    "contrastive_aug_embeddings_dim_250 = np.load(\"aug_embeddings/aug_song_embeddings_dim_250.npy\")\n",
    "contrastive_aug_embeddings_dim_500 = np.load(\"aug_embeddings/aug_song_embeddings_dim_500.npy\")\n",
    "\n",
    "train_contrastive(contrastive_aug_embeddings_dim_250, threshold, bestMargin)\n",
    "train_contrastive(contrastive_aug_embeddings_dim_500, threshold, bestMargin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save embeddings to file for easier uploading next time. \n",
    "\"\"\"\n",
    "os.makedirs(\"cont_embeddings\", exist_ok=True)\n",
    "np.save(f\"cont_embeddings/cont_song_embeddings_dim_{250}.npy\", contrastive_song_embeddings_dim_250)\n",
    "print(\"Embeddings saved to \\'cont_embeddings/\\' directory.\")\n",
    "np.save(f\"cont_embeddings/cont_song_embeddings_dim_{500}.npy\", contrastive_song_embeddings_dim_500)\n",
    "print(\"Embeddings saved to \\'cont_embeddings/\\' directory.\")\n",
    "\n",
    "os.makedirs(\"cont_aug_embeddings\", exist_ok=True)\n",
    "np.save(f\"cont_aug_embeddings/cont_aug_song_embeddings_dim_{250}.npy\", contrastive_aug_embeddings_dim_250)\n",
    "print(\"Embeddings saved to \\'cont_aug_embeddings/\\' directory.\")\n",
    "np.save(f\"cont_aug_embeddings/cont_aug_song_embeddings_dim_{500}.npy\", contrastive_aug_embeddings_dim_500)\n",
    "print(\"Embeddings saved to \\'cont_embeddings/\\' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate the contrastive models, then compare with the other existing models.\n",
    "\"\"\"\n",
    "contrastive_song_embeddings_dim_250 = np.load(f\"cont_embeddings/cont_song_embeddings_dim_250.npy\")\n",
    "print(\"Contrastive embeddings dim 250:\")\n",
    "get_val_accuracy(contrastive_song_embeddings_dim_250)\n",
    "\n",
    "contrastive_song_embeddings_dim_500 = np.load(f\"cont_embeddings/cont_song_embeddings_dim_500.npy\")\n",
    "print(\"Contrastive embeddings dim 500:\")\n",
    "get_val_accuracy(contrastive_song_embeddings_dim_500)\n",
    "\n",
    "contrastive_aug_embeddings_dim_250 = np.load(f\"cont_aug_embeddings/cont_aug_song_embeddings_dim_250.npy\")\n",
    "print(\"Contrastive Augmented embeddings dim 250:\")\n",
    "get_val_accuracy(contrastive_aug_embeddings_dim_250)\n",
    "\n",
    "contrastive_song_embeddings_dim_500 = np.load(f\"cont_aug_embeddings/cont_aug_song_embeddings_dim_500.npy\")\n",
    "print(\"Contrastive Augmented embeddings dim 500:\")\n",
    "get_val_accuracy(contrastive_aug_embeddings_dim_500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
